---
slug: "/blog/fc131408b78fb50e8269"
title: Rails3系でのssl検証 - ブラウザのsslは無効にできない？ - 
date: 2014-05-07T23:07:13+09:00
tags: ["Rails", "SSL"]
---

<h3>
<span id="背景" class="fragment"></span><a href="#%E8%83%8C%E6%99%AF"><i class="fa fa-link"></i></a>背景</h3>

<hr>

<p>Railsでsslを常に使ってて、ドメイン内で一部sslを適用したくないところがあり、<br><br>
いろいろ検証してみました。<br><br>
とりあえずsslを無効にしてアクセスしてみようとしたらハマった。  </p>

<h3>
<span id="メモ" class="fragment"></span><a href="#%E3%83%A1%E3%83%A2"><i class="fa fa-link"></i></a>メモ</h3>

<hr>

<p>最近のブラウザでtlsの無効/有効を設定できるけど<br><br>
sslを無効にするやり方がみつからない。  </p>

<p>ブラウザのバージョン下げるのもめんどうなので、CUIでやっちゃおう。</p>

<p>wgetのヘルプをみると、、、  </p>

<div class="code-frame" data-lang="text"><div class="highlight"><pre>GNU Wget 1.13.4, 非対話的ネットワーク転送ソフト
使い方: wget [オプション]... [URL]...

長いオプションで不可欠な引数は短いオプションでも不可欠です。

スタートアップ:
  -V,  --version           バージョン情報を表示して終了する
  -h,  --help              このヘルプを表示する
  -b,  --background        スタート後にバックグラウンドに移行する
  -e,  --execute=COMMAND   `.wgetrc'形式のコマンドを実行する

ログと入力ファイル:
  -o,  --output-file=FILE    ログを FILE に出力する
  -a,  --append-output=FILE  メッセージを FILE に追記する
  -d,  --debug               デバッグ情報を表示する
  -q,  --quiet               何も出力しない
  -v,  --verbose             冗長な出力をする (デフォルト)
  -nv, --no-verbose          冗長ではなくする
  -i,  --input-file=FILE     FILE の中に指定された URL をダウンロードする
  -F,  --force-html          入力ファイルを HTML として扱う
  -B,  --base=URL            HTML で入力されたファイル(-i -F)のリンクを
                             指定した URL の相対 URL として扱う
       --config=FILE         設定ファイルを指定する

ダウンロード:
  -t,  --tries=NUMBER            リトライ回数の上限を指定 (0 は無制限).
       --retry-connrefused       接続を拒否されてもリトライする
  -O,  --output-document=FILE    FILE に文書を書きこむ
  -nc, --no-clobber              存在しているファイルをダウンロードで上書きしない
  -c,  --continue                部分的にダウンロードしたファイルの続きから始める
       --progress=TYPE           進行表示ゲージの種類を TYPE に指定する
  -N,  --timestamping            ローカルにあるファイルよりも新しいファイルだけ取得する
  --no-use-server-timestamps     ローカル側のファイルのタイムスタンプに
                                 サーバのものを使わない
  -S,  --server-response         サーバの応答を表示する
       --spider                  何もダウンロードしない
  -T,  --timeout=SECONDS         全てのタイムアウトを SECONDS 秒に設定する
       --dns-timeout=SECS        DNS 問い合わせのタイムアウトを SECS 秒に設定する
       --connect-timeout=SECS    接続タイムアウトを SECS 秒に設定する
       --read-timeout=SECS       読み込みタイムアウトを SECS 秒に設定する
  -w,  --wait=SECONDS            ダウンロード毎に SECONDS 秒待つ
       --waitretry=SECONDS       リトライ毎に 1〜SECONDS 秒待つ
       --random-wait             ダウンロード毎に 0.5*WAIT〜1.5*WAIT 秒待つ
       --no-proxy                プロクシを使わない
  -Q,  --quota=NUMBER            ダウンロードするバイト数の上限を指定する
       --bind-address=ADDRESS    ローカルアドレスとして ADDRESS (ホスト名か IP) を使う
       --limit-rate=RATE         ダウンロード速度を RATE に制限する
       --no-dns-cache            DNS の問い合わせ結果をキャッシュしない
       --restrict-file-names=OS  OS が許しているファイル名に制限する
       --ignore-case             ファイル名/ディレクトリ名の比較で大文字小文字を無視する
  -4,  --inet4-only              IPv4 だけを使う
  -6,  --inet6-only              IPv6 だけを使う
       --prefer-family=FAMILY    指定したファミリ(IPv6, IPv4, none)で最初に接続する
       --user=USER               ftp, http のユーザ名を指定する
       --password=PASS           ftp, http のパスワードを指定する
       --ask-password            パスワードを別途入力する
       --no-iri                  IRI サポートを使わない
       --local-encoding=ENC      指定した ENC を IRI のローカルエンコーディングにする
       --remote-encoding=ENC     指定した ENC をデフォルトのリモートエンコーディングにする
       --unlink                  上書きする前にファイルを削除する

ディレクトリ:
  -nd, --no-directories           ディレクトリを作らない
  -x,  --force-directories        ディレクトリを強制的に作る
  -nH, --no-host-directories      ホスト名のディレクトリを作らない
       --protocol-directories     プロトコル名のディレクトリを作る
  -P,  --directory-prefix=PREFIX  ファイルを PREFIX/ 以下に保存する
       --cut-dirs=NUMBER          リモートディレクトリ名の NUMBER 階層分を無視する

HTTP オプション:
       --http-user=USER        http ユーザ名として USER を使う
       --http-password=PASS    http パスワードとして PASS を使う
       --no-cache              サーバがキャッシュしたデータを許可しない
       --default-page=NAME     デフォルトのページ名を NAME に変更します
                               通常は `index.html' です
  -E,  --adjust-extension      HTML/CSS 文書は適切な拡張子で保存する
       --ignore-length         `Content-Length' ヘッダを無視する
       --header=STRING         送信するヘッダに STRING を追加する
       --max-redirect          ページで許可する最大転送回数
       --proxy-user=USER       プロクシユーザ名として USER を使う
       --proxy-password=PASS   プロクシパスワードとして PASS を使う
       --referer=URL           Referer を URL に設定する
       --save-headers          HTTP のヘッダをファイルに保存する
  -U,  --user-agent=AGENT      User-Agent として Wget/VERSION ではなく AGENT を使う
       --no-http-keep-alive    HTTP の keep-alive (持続的接続) 機能を使わない
       --no-cookies            クッキーを使わない
       --load-cookies=FILE     クッキーを FILE から読みこむ
       --save-cookies=FILE     クッキーを FILE に保存する
       --keep-session-cookies  セッションだけで用いるクッキーを保持する
       --post-data=STRING      POST メソッドを用いて STRING を送信する
       --post-file=FILE        POST メソッドを用いて FILE の中味を送信する
       --content-disposition   Content-Disposition ヘッダがあれば
                               ローカルのファイル名として用いる (実験的)
       --auth-no-challenge     サーバからのチャレンジを待たずに、
                               Basic認証の情報を送信します。

HTTPS (SSL/TLS) オプション:
       --secure-protocol=PR     セキュアプロトコルを選択する (auto, SSLv2, SSLv3, TLSv1)
       --no-check-certificate   サーバ証明書を検証しない
       --certificate=FILE       クライアント証明書として FILE を使う
       --certificate-type=TYPE  クライアント証明書の種類を TYPE (PEM, DER) に設定する
       --private-key=FILE       秘密鍵として FILE を使う
       --private-key-type=TYPE  秘密鍵の種類を TYPE (PEM, DER) に設定する
       --ca-certificate=FILE    CA 証明書として FILE を使う
       --ca-directory=DIR       CA のハッシュリストが保持されているディレクトリを指定する
       --random-file=FILE       SSL PRNG の初期化データに使うファイルを指定する
       --egd-file=FILE          EGD ソケットとして FILE を使う

FTP オプション:
       --ftp-user=USER         ftp ユーザとして USER を使う
       --ftp-password=PASS     ftp パスワードとして PASS を使う
       --no-remove-listing     `.listing' ファイルを削除しない
       --no-glob               FTP ファイル名のグロブを無効にする
       --no-passive-ftp        "passive" 転送モードを使わない
       --retr-symlinks         再帰取得中に、シンボリックリンクでリンクされた先のファイルを取得する

再帰ダウンロード:
  -r,  --recursive          再帰ダウンロードを行う
  -l,  --level=NUMBER       再帰時の階層の最大の深さを NUMBER に設定する (0 で無制限)
       --delete-after       ダウンロード終了後、ダウンロードしたファイルを削除する
  -k,  --convert-links      HTML や CSS 中のリンクをローカルを指すように変更する
  -K,  --backup-converted   リンク変換前のファイルを .orig として保存する
  -m,  --mirror             -N -r -l 0 --no-remove-listing の省略形
  -p,  --page-requisites    HTML を表示するのに必要な全ての画像等も取得する
       --strict-comments    HTML 中のコメントの処理を厳密にする

再帰ダウンロード時のフィルタ:
  -A,  --accept=LIST               ダウンロードする拡張子をコンマ区切りで指定する
  -R,  --reject=LIST               ダウンロードしない拡張子をコンマ区切りで指定する
  -D,  --domains=LIST              ダウンロードするドメインをコンマ区切りで指定する
       --exclude-domains=LIST      ダウンロードしないドメインをコンマ区切りで指定する
       --follow-ftp                HTML 文書中の FTP リンクも取得対象にする
       --follow-tags=LIST          取得対象にするタグ名をコンマ区切りで指定する
       --ignore-tags=LIST          取得対象にしないタグ名をコンマ区切りで指定する
  -H,  --span-hosts                再帰中に別のホストもダウンロード対象にする
  -L,  --relative                  相対リンクだけ取得対象にする
  -I,  --include-directories=LIST  取得対象にするディレクトリを指定する
  --trust-server-names  ファイル名としてリダイレクト先のURLの最後の部分を使う
  -X,  --exclude-directories=LIST  取得対象にしないディレクトリを指定する
  -np, --no-parent                 親ディレクトリを取得対象にしない

バグ報告や提案は&amp;lt;bug-wget@gnu.org&amp;gt;へ
</pre></div></div>

<p>うーん、できなさそう。。。<br><br>
getできるか見たいだけだし、もうsslが無効のプロキシサーバを使ってみよう。  </p>

<p><a href="http://www.freeproxylists.net/ja/" rel="nofollow noopener" target="_blank">ここ</a>をつかって<br><br>
<a href="http://sitedo3.s3.amazonaws.com/2013/09/freeproxylist.png" rel="nofollow noopener" target="_blank"><img src="http://sitedo3.s3.amazonaws.com/2013/09/freeproxylist-300x191.png" alt="freeproxylist" width="300" height="191" data-canonical-src="http://sitedo3.s3.amazonaws.com/2013/09/freeproxylist-300x191.png" loading="lazy"></a></p>

<div class="code-frame" data-lang="text"><div class="highlight"><pre>wget -e 'http_proxy=[proxyサーバのアドレス]' xxx.com

スパイダーモードが有効です。リモートファイルが存在してるか確認します。
--2013-09-26 13:33:06--  http://box2you.com/
31.170.178.2:8080 に接続しています... 接続しました。
Proxy による接続要求を送信しました、応答を待っています... 301 Moved Permanently
場所: https://www.xxxxx.com/ [続く]
スパイダーモードが有効です。リモートファイルが存在してるか確認します。
--2013-09-26 13:33:08--  https://www.xxxxx.com/
www.xxxxx.com (www.xxxxx.com) をDNSに問いあわせています...xxx.xxx.xxx.
www.xxxxx.com (www.xxxxx.com)|xxxx|:443 に接続しています... 接続しました。
HTTP による接続要求を送信しました、応答を待っています... 200 OK
長さ: 33221 (32K) [text/html]
リモートファイルが存在し、さらなるリンクもあり得ますが、再帰が禁止されています -- 取得しません。
</pre></div></div>

<p>だめじゃん。ポート443でhttpsしてんじゃん。  </p>

<p>もはやクライアントではsslが有効なのは常識なのか、、、<br><br>
サーバサイドだけ考えればいいのね。じゃ、この検証はスルー、、、<strong>できるかい！！</strong></p>

<p>ここまできたら気になるわ。<br><br>
めんどいけどhttpクライアントを使ってやろう。  </p>

<p>下のようにrubyのgem'faraday'を使ってやりました。  </p>

<div class="code-frame" data-lang="text"><div class="highlight"><pre>pry(main)&gt; Faraday::Connection.new('http://box2you.com',ssl:{verify:true}).get
=&gt; #&lt;Faraday::Response:0x00000002de94f8
 @env=
  {:method=&gt;:get,
   :body=&gt;
    "Redirecting to &lt;a href=\"https://www.box2you.com/\"&gt;https://www.box2you.com/&lt;/a&gt;",
   :url=&gt;#&lt;URI::HTTP:0x00000002dd90d0 URL:http://box2you.com/&gt;,
   :request_headers=&gt;{"User-Agent"=&gt;"Faraday v0.8.7"},
   :parallel_manager=&gt;nil,
   :request=&gt;{:proxy=&gt;nil},
   :ssl=&gt;{:verify=&gt;true},
   :status=&gt;301,
   :response_headers=&gt;
    {"server"=&gt;"nginx",
     "date"=&gt;"Thu, 26 Sep 2013 07:05:20 GMT",
     "content-type"=&gt;"application/x-msdownload",
     "content-length"=&gt;"78",
     "connection"=&gt;"close",
     "location"=&gt;"https://www.box2you.com/"},
   :response=&gt;#&lt;Faraday::Response:0x00000002de94f8 ...&gt;},
 @on_complete_callbacks=[]&gt;
</pre></div></div>

<p>うーん？思ってたのと違うけど、できた。  </p>

<p>どうやらRailsで全リソースをSSL対応させると、<br><br>
httpのみでアクセスしたときもhttpsにリダイレクトされるみたい。  </p>

<p>これは困った。<br><br>
下の記事を参考にアクション毎にコントロールするか。。。。  </p>

<p><a href="http://o.inchiki.jp/obbr/24" rel="nofollow noopener" target="_blank">RailsでSSLを利用するときのあれこれ</a></p>
